/*--------------------------------------------------------------------------
   Author: Thomas Nowotny
  
   Institute: Center for Computational Neuroscience and Robotics
              University of Sussex
	      Falmer, Brighton BN1 9QJ, UK 
  
   email to:  T.Nowotny@sussex.ac.uk
  
   initial version: 2010-02-07
  
--------------------------------------------------------------------------*/

//-----------------------------------------------------------------------
/*!  \file generateCuda.cpp 
  
  \brief Contains functions to generate code for running the simulation on 
  the GPU, and for I/O convenience functions between GPU
  and CPU space. Part of the code generation section.
*/
//--------------------------------------------------------------------------

//----------------------------------------------------------------------------
/*!
  \brief A function to generate CUDA device-side code.

  The function generates functions that will spawn kernel grids onto the GPU
  (but not the actual kernel code which is generated in "genNeuronKernel()"
  and "genSynpaseKernel()"). Generated functions include "copyGToDevice()",
  "copyGFromDevice()", "copyStateToDevice()", "copyStateFromDevice()",
  "copySpikesFromDevice()", "copySpikeNFromDevice()" and "stepTimeGPU()". The 
  last mentioned function is the function that will initialize the execution
  on the GPU in the generated simulation engine. All other generated functions
  are "convenience functions" to handle data transfer from and to the GPU.
*/
//----------------------------------------------------------------------------

void genCudaCode(int deviceID, ostream &mos)
{
  mos << "entering genCudaCode" << endl;
  string cppFile, hppFile;
  ofstream os, osh;
  unsigned int type, size, trgN, mem = 0;


  //----------------------------------
  // open and setup cuda[deviceID].hpp

  hppFile = path + tS("/") + model->name + tS("_CODE_CUDA_") + tS(deviceID) + tS("/cuda") + tS(deviceID) + tS(".hpp");
  osh.open(hppFile.c_str());
  writeHeader(osh);
  osh << endl;
  osh << "//-------------------------------------------------------------------------" << endl;
  osh << "/*! \\file cuda" << deviceID << ".hpp" << endl << endl;
  osh << "\\brief File generated by GeNN for the model " << model->name << " containing CUDA declarations." << endl;
  osh << "*/" << endl;
  osh << "//-------------------------------------------------------------------------" << endl << endl;


  //--------------------------
  // open and setup control.cu

  cppFile = path + tS("/") + model->name + tS("_CODE_CUDA_") + tS(deviceID) + tS("/control.cu");
  os.open(cppFile.c_str());
  writeHeader(os);
  os << endl;
  os << "//-------------------------------------------------------------------------" << endl;
  os << "/*! \\file control.cu" << endl << endl;
  os << "\\brief File generated by GeNN for the model " << model->name << " containing CUDA control code." << endl;
  os << "*/" << endl;
  os << "//-------------------------------------------------------------------------" << endl << endl;


  //------------------------
  // add control.cu includes

  os << "#include \"utils.h\"" << endl;
  os << "#include \"cuda" << deviceID << ".hpp\"" << endl;
  os << "#include \"neuron.cu\"" << endl;
  if (model->synapseGrpN>0) os << "#include \"synapse.cu\"" << endl;
  os << "#ifndef RAND" << endl;
  os << "#define RAND(Y, X) Y = Y * 1103515245 + 12345; X = (unsigned int) (Y >> 16) & 32767" << endl;
  os << "#endif" << endl;
  os << endl;


  //------------------------
  // device neuron variables

  for (int i = 0; i < model->neuronGrpN; i++) {
    type = model->neuronType[i];
    osh << "// device " << model->neuronName[i] << " neuron group variables" << endl;
    for (int j = 0; j < nModels[type].varNames.size(); j++) {
      osh << nModels[type].varTypes[j] << " *" << "d_" << nModels[type].varNames[j] << model->neuronName[i] << ";" << endl;
    }
  }


  //-------------------------
  // device synapse variables

  for (int i = 0; i< model->postSynapseType.size(); i++){
    type = model->postSynapseType[i];
    osh << "// device " << model->neuronName[i] << " synapse group variables" << endl;
    for (int j = 0; j < postSynModels[type].varNames.size(); j++) {
      osh << postSynModels[type].varTypes[j] << " *d_" << postSynModels[type].varNames[j] << model->synapseName[i] << ";" << endl;
      // should work at the moment but if we make postsynapse vectors independent of synapses this may change
    }
  }
  for (int i = 0; i < model->synapseGrpN; i++) {
    if (model->synapseGType[i] == INDIVIDUALG) {
      osh << model->ftype << " *d_gp" << model->synapseName[i] << ";" << endl;
      if (model->synapseType[i] == LEARN1SYNAPSE) {
	osh << model->ftype << " *d_grawp" << model->synapseName[i] << ";" << endl;
      }
    }
    if (model->synapseGType[i] == INDIVIDUALID) {
      osh << "unsigned int *d_gp" << model->synapseName[i] << ";" << endl;
    }
    if (model->synapseConnType[i] == SPARSE) {
      osh << "unsigned int *d_gp" << model->synapseName[i] << "_indInG;" << endl;
      osh << "unsigned int *d_gp" << model->synapseName[i] << "_ind;" << endl;
      trgN = model->neuronN[model->synapseTarget[i]];
    } 
  }
  osh << endl;


  //----------------------------
  // device block and grid sizes

  if (model->synapseGrpN > 0) { 
    unsigned int synapseGridSz = model->padSumSynapseKrnl[model->synapseGrpN - 1];   
    synapseGridSz = synapseGridSz / synapseBlkSz[deviceID];
    osh << "dim3 sThreadsCuda" << deviceID << "(" << synapseBlkSz[deviceID] << ", 1);" << endl;
    osh << "dim3 sGridCuda" << deviceID << "(" << synapseGridSz << ", 1);" << endl;
    osh << endl;
  }
  if (model->lrnGroups > 0) {
    unsigned int learnGridSz = model->padSumLearnN[model->lrnGroups - 1];
    learnGridSz = learnGridSz / learnBlkSz[deviceID];
    osh << "dim3 lThreadsCuda" << deviceID << "(" << learnBlkSz[deviceID] << ", 1);" << endl;
    osh << "dim3 lGridCuda" << deviceID << "(" << learnGridSz << ", 1);" << endl;
    osh << endl;
  }
  unsigned int neuronGridSz = model->padSumNeuronN[model->neuronGrpN - 1];
  neuronGridSz = neuronGridSz / neuronBlkSz[deviceID];
  osh << "dim3 nThreadsCuda" << deviceID << "(" << neuronBlkSz[deviceID] << ", 1);" << endl;
  if (neuronGridSz < deviceProp[deviceID].maxGridSize[1]) {
    osh << "dim3 nGridCuda" << deviceID << "(" << neuronGridSz << ", 1);" << endl;
  }
  else {
    int sqGridSize = ceil(sqrt(neuronGridSz));
    osh << "dim3 nGridCuda" << deviceID << "(" << sqGridSize << ", " << sqGridSize << ");" << endl;
  }
  osh << endl;



















  // BELOW: needs sorting to find only cuda memory allocation



  //----------------------------------------------------
  // Code for setting the CUDA device's global variables
  // Also estimating memory usage on device ...

  os << "void allocateMemCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  //os << "  " << model->ftype << " free_m,total_m;" << endl;
  //os << "  cudaMemGetInfo((size_t*)&free_m,(size_t*)&total_m);" << endl;
  os << "  size_t size;" << endl;
  for (int i = 0; i < model->neuronGrpN; i++) {
    nt = model->neuronType[i];
    os << "  glbSpk" << model->neuronName[i] << " = new unsigned int[" << model->neuronN[i] << "];" << endl;
    if (model->neuronDelaySlots[i] == 1) {
      os << "  glbSpkEvnt" << model->neuronName[i] << " = new unsigned int[" << model->neuronN[i] << "];" << endl;
      mem += model->neuronN[i] * sizeof(unsigned int);
    }
    else {
      os << "  glbSpkEvntCnt" << model->neuronName[i] << " = new unsigned int[" << model->neuronDelaySlots[i] << "];" << endl;
      os << "  glbSpkEvnt" << model->neuronName[i] << " = new unsigned int[" << model->neuronN[i] * model->neuronDelaySlots[i] << "];" << endl;
      mem += model->neuronN[i] * model->neuronDelaySlots[i] * sizeof(unsigned int);
    }
    for (int j = 0; j < model->inSyn[i].size(); j++) {
      os << "  inSyn" << model->neuronName[i] << j << " = new " << model->ftype << "[";
      os << model->neuronN[i] << "];" << endl;
      mem += model->neuronN[i] * theSize(model->ftype);
    } 
    for (int j = 0; j < nModels[nt].varNames.size(); j++) {
      os << "  " << nModels[nt].varNames[j] << model->neuronName[i] << " = new " << nModels[nt].varTypes[j] << "[";
      if ((nModels[nt].varNames[j] == "V") && (model->neuronDelaySlots[i] != 1)) {
	os << (model->neuronDelaySlots[i] * model->neuronN[i]);
	mem += (model->neuronDelaySlots[i] * model->neuronN[i] * sizeof(nModels[nt].varTypes[j]));
      }
      else {
	os << (model->neuronN[i]);
	mem += (model->neuronN[i] * sizeof(nModels[nt].varTypes[j]));
      }
      os << "];" << endl;
    }
    if (model->neuronNeedSt[i]) {
      os << "  sT" << model->neuronName[i] << " = new " << model->ftype << "[" << model->neuronN[i] << "];" << endl;
    }   
    
    // allocate device neuron variables
    for (int j = 0; j < nModels[nt].varNames.size(); j++) {
      os << "  size = sizeof(" << nModels[nt].varTypes[j] << ") * ";
      if ((nModels[nt].varNames[j] == "V") && (model->neuronDelaySlots[i] != 1)) {
	os << model->neuronN[i] * model->neuronDelaySlots[i] << ";" << endl;
      }
      else {
	os << model->neuronN[i] << ";" << endl;
      }
      os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_" << nModels[nt].varNames[j] << model->neuronName[i] << ", size));" << endl;
    }
    os << endl; 
  }
  for (int i = 0; i < model->synapseGrpN; i++) {
    if (model->synapseGType[i] == INDIVIDUALG) {
      // if (model->synapseConnType[i] == SPARSE) {
      /*********************If sparse, the arrays will be allocated later. ****************/
      //mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype); //TODO: This is actually less for sparse matrices but we need to find a way      
      //mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * sizeof(int);
      //}
      //else {
      if (model->synapseConnType[i] != SPARSE) { 
	os << "  gp" << model->synapseName[i] << " = new " << model->ftype << "[";
	os << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]];
      	os << "];      // synaptic conductances of group " << model->synapseName[i] << endl;
      	mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype);
      }
      if (model->synapseType[i] == LEARN1SYNAPSE) {
	os << "  grawp" << model->synapseName[i] << " = new " << model->ftype << "[";
	os << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]];
	os << "]; // raw synaptic conductances of group " << model->synapseName[i] << endl;
	mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype);
      }
    }
    // note, if GLOBALG we put the value at compile time
    if (model->synapseGType[i] == INDIVIDUALID) {
      os << "  gp" << model->synapseName[i] << " = new unsigned int[";
      unsigned long int tmp = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]];
      unsigned long int size = tmp >> logUIntSz;
      if (tmp > (size << logUIntSz)) size++;
      os << size << "]; // synaptic connectivity of group " << model->synapseName[i] << endl;
      mem += size * sizeof(unsigned int);
    }
    if (model->synapseGType[i] == INDIVIDUALG) {
      // (cases necessary here when considering sparse reps as well)
      //os << "  size =" << model->neuronN[model->synapseSource[i]] << "*" << model->neuronN[model->synapseTarget[i]] << "; " << endl;
      //os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << ", sizeof(" << model->ftype << ")*size);" << endl;
      /*if (model->synapseConnType[i]==SPARSE){
	os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << "_ind, sizeof(unsigned int)*size);" << endl;
	os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << "_indInG, sizeof(unsigned int)*("<< model->neuronN[model->synapseSource[i]] << "+1));" << endl;
	os << "  size = sizeof(" << model->ftype << ")*" << "  g" << model->synapseName[i] << ".connN; " << endl;
	}
	else{*/
      if (model->synapseConnType[i]!=SPARSE) {
	os << "  size = sizeof(" << model->ftype << ") * " << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]] << "; " << endl;
	os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_gp" << model->synapseName[i] << ", size));" << endl;
	if (model->ftype == "float") {
	  mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * sizeof(float);
	}
	else {
	  mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * sizeof(double);
	}
      }
      if (model->synapseType[i] == LEARN1SYNAPSE) {
	os << "  size = sizeof(" << model->ftype << ") * " << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]] << "; " << endl; // not sure if correct				
	os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_grawp" << model->synapseName[i] << ", size)); // raw synaptic conductances of group " << model->synapseName[i] << endl;
	if (model->ftype == "float") {
	  mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * sizeof(float);
	}
	else {
	  mem += model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * sizeof(double); 
	}
      }
    }
    // note, if GLOBALG we put the value at compile time
    if (model->synapseGType[i] == INDIVIDUALID) {
      unsigned int tmp = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]];
      os << "  size = sizeof(unsigned int) * " << tmp << ";" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_gp" << model->synapseName[i] << ",";      
      unsigned int size = tmp >> logUIntSz;
      if (tmp > (size << logUIntSz)) size++;
      os << size << ")); // synaptic connectivity of group " << model->synapseName[i] << endl;
      mem += size;
    }
  }  
  for (int i = 0; i < model->postSynapseType.size(); i++) {
    int pst = model->postSynapseType[i];
    for (int j = 0; j < postSynModels[pst].varNames.size(); j++) {
      os << "  " << postSynModels[pst].varNames[j] << model->synapseName[i] << " = new " << postSynModels[pst].varTypes[j] << "[" << (model->neuronN[model->synapseTarget[i]]) <<  "];" << endl;
    }
  }
  os << "}" << endl;
  os << endl;
  


















  os << "void allocateAllSparseArraysCuda" << deviceID << "() {" << endl;
  for (int i = 0; i < model->synapseGrpN; i++) {
    if (model->synapseConnType[i] == SPARSE) {
      if (model->synapseGType[i] != GLOBALG) os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_gp" << model->synapseName[i]<< ", sizeof(" << model->ftype << ") * g" << model->synapseName[i] << ".connN));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_gp" << model->synapseName[i] << "_ind, sizeof(unsigned int) * g" << model->synapseName[i] << ".connN));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMalloc((void **) &d_gp" << model->synapseName[i] << "_indInG, sizeof(unsigned int) * (" << model->neuronN[model->synapseSource[i]] << " + 1)));" << endl;
      //mem += gsize * theSize(model->ftype); //TODO: We don't know connN at code generation step. But we need to find a way.
      mem += model->neuronN[model->synapseSource[i]]*1000*sizeof(float);//!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!CHANGE THAT BEFORE COMMIT
      mem += model->neuronN[model->synapseSource[i]]*1000*sizeof(unsigned int);//!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!CHANGE THAT BEFORE COMMIT
      /*trgN = model->neuronN[model->synapseTarget[i]];
	rrr if (trgN > neuronBlkSz[deviceID]) {
	os <<  "  cudaMalloc((void **) &d_Lg" << model->synapseName[i] << ",sizeof(" << model->ftype <<")*"<<  trgN << ");" << endl;
        }*/
    }
  }
  os << "}" << endl; 
  os << endl;

  // ------------------------------------------------------------------------
  // allocating conductance arrays for sparse matrices
  /*
    for (int i= 0; i < model->synapseGrpN; i++) {
    if (model->synapseConnType[i] == SPARSE){
    os << "void allocateSparseArray" << model->synapseName[i] << "(unsigned int i, unsigned int gsize)" << endl; //i=synapse index
    os << "{" << endl;
    os << "  g" << model->synapseName[i] << ".gp= new " << model->ftype << "[gsize];" << endl; // synaptic conductances of group " << model->synapseName[i];
    //mem += gsize * theSize(model->ftype); //TODO: But we need to find a way
    os << "  g" << model->synapseName[i] << ".gIndInG= new unsigned int[";
    os << model->neuronN[model->synapseSource[i]] << "+ 1];"; // index where the next neuron starts in the synaptic conductances of group " << model->synapseName[i];
    os << endl;
    mem+= model->neuronN[model->synapseSource[i]]*sizeof(int);
    os << "  g" << model->synapseName[i] << ".gInd= new unsigned int[gsize];" << endl; // postsynaptic neuron index in the synaptic conductances of group " << model->synapseName[i];
    //mem += gsize * sizeof(int);
    //  }
    os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << ", sizeof(" << model->ftype << ")*gsize);" << endl;
    os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << "_ind, sizeof(unsigned int)*gsize);" << endl;
    os << "  cudaMalloc((void **)&d_gp" << model->synapseName[i] << "_indInG, sizeof(unsigned int)*("<< model->neuronN[model->synapseSource[i]] << "+1));" << endl;
    os << "}" << endl; 
    }}*/
















  // INITIALISE














  //-------------------------------
  // copying conductances to device

  os << "void copyGToCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  for (int i= 0; i < model->synapseGrpN; i++) {
    if (model->synapseGType[i] == INDIVIDUALG) {
      if (model->synapseConnType[i] == SPARSE){          
	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_gp" << model->synapseName[i] << ", g" << model->synapseName[i];
	os << ".gp, g" << model->synapseName[i] << ".connN*sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
      }
      else {
	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_gp" << model->synapseName[i] << ", gp" << model->synapseName[i];
      	size= model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]];
      	os << ", " << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]]<< " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
      }
      if (model->synapseType[i] == LEARN1SYNAPSE) {
        os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_grawp" << model->synapseName[i]<< ", grawp" << model->synapseName[i];
	if (model->synapseConnType[i] == SPARSE) {
          os << ", " << model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] << " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;}
	else {
          os << ", " << model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] << " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
	}
      } 
    }
    if (model->synapseGType[i] == INDIVIDUALID) {
      if (model->synapseConnType[i] == SPARSE) {
      	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_gp" << model->synapseName[i] << ", g" << model->synapseName[i];
	os << ".gp, ";
      }
      else {
	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_gp" << model->synapseName[i] << ", gp" << model->synapseName[i];
	os << ", ";
      }
      unsigned int tmp = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]];
      size = (tmp >> logUIntSz);
      if (tmp > (size << logUIntSz)) size++;
      size = size * sizeof(unsigned int);
      os << "," << size << ", cudaMemcpyHostToDevice));" << endl;
      if (model->synapseType[i] == LEARN1SYNAPSE) {
        os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_grawp" << model->synapseName[i]<< ", grawp" << model->synapseName[i];
	if (model->synapseConnType[i]==SPARSE) {
          os << ", " << model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] << " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
	}
	else {
          os << ", " << model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] << " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
	}
      }
    }
  }
  os << "}" << endl;
  os << endl;


  // ------------------------------------------------------------------------
  // copying explicit input(if any) to device
  /*
    os << "void copyInpToDevice()" << endl;
    os << "{" << endl;
    os << "}" << endl;
    os << endl;*/


  // ------------------------------------------------------------------------
  // copying conductances from device

  os << "void copyGFromCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  for (int i= 0; i < model->synapseGrpN; i++) {
    if (model->synapseGType[i] == INDIVIDUALG) {
      if (model->synapseConnType[i]==SPARSE){
	//size = model->neuronN[model->synapseSource[i]]*model->neuronN[model->synapseTarget[i]];
	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(g" << model->synapseName[i] << ".gp, d_gp" << model->synapseName[i];
	os << ", g" << model->synapseName[i] << ".connN * sizeof(" << model->ftype << "), cudaMemcpyDeviceToHost));" << endl;
      }
      else {
	os << "  CHECK_CUDA_ERRORS(cudaMemcpy(gp" << model->synapseName[i] << ", d_gp" << model->synapseName[i];      
      	size = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype);
	os << ", " << model->neuronN[model->synapseSource[i]] << " * " << model->neuronN[model->synapseTarget[i]]<< " * sizeof(" << model->ftype << "), cudaMemcpyDeviceToHost));" << endl;
      }  
      if (model->synapseType[i] == LEARN1SYNAPSE) {
        os << "  CHECK_CUDA_ERRORS(cudaMemcpy(grawp" << model->synapseName[i]<< ", d_grawp" << model->synapseName[i];
        size = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype);
        os << "," << size << ", cudaMemcpyDeviceToHost));" << endl;
      }
    }
    if (model->synapseGType[i] == INDIVIDUALID) {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(gp" << model->synapseName[i] << ", d_gp" << model->synapseName[i];
      unsigned int tmp = model->neuronN[model->synapseSource[i]]*model->neuronN[model->synapseTarget[i]];
      size = (tmp >> logUIntSz);
      if (tmp > (size << logUIntSz)) size++;
      size = size * sizeof(unsigned int);
      os << ", " << size << ", cudaMemcpyDeviceToHost));" << endl;
      if (model->synapseType[i] == LEARN1SYNAPSE) {
        os << "  CHECK_CUDA_ERRORS(cudaMemcpy(grawp" << model->synapseName[i]<< ", d_grawp" << model->synapseName[i];
        size = model->neuronN[model->synapseSource[i]] * model->neuronN[model->synapseTarget[i]] * theSize(model->ftype);
        os << "," << size << ", cudaMemcpyDeviceToHost));" << endl;
      }
    }
  }
  os << "}" << endl;
  os << endl;


  // ------------------------------------------------------------------------
  // copying particular conductances group from device
  // ------------------------------------------------------------------------

  // ------------------------------------------------------------------------
  // copying values to device

  os << "void copyStateToCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  os << "  void *devPtr;" << endl;
  os << "  unsigned int tmp= 0;" << endl;
  os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_done));" << endl;
  os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, &tmp, sizeof(int), cudaMemcpyHostToDevice));" << endl;
  for (int i= 0; i < model->neuronGrpN; i++) {
    nt= model->neuronType[i];
    if (model->neuronDelaySlots[i] != 1) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_spkEvntQuePtr" << model->neuronName[i] << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, &spkEvntQuePtr" << model->neuronName[i] << ", ";
      size = sizeof(unsigned int);
      os << size << ", cudaMemcpyHostToDevice));" << endl;
    }
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbscnt" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, &glbscnt" << model->neuronName[i] << ", ";
    size = sizeof(unsigned int);
    os << size << ", cudaMemcpyHostToDevice));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpkEvntCnt" << model->neuronName[i] << "));" << endl;
    if (model->neuronDelaySlots[i] == 1) {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, &glbSpkEvntCnt" << model->neuronName[i] << ", ";
      size = sizeof(unsigned int);
    }
    else {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, glbSpkEvntCnt" << model->neuronName[i] << ", ";
      size = model->neuronDelaySlots[i] * sizeof(unsigned int);
    }
    os << size << ", cudaMemcpyHostToDevice));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpk" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, glbSpk" << model->neuronName[i] << ", ";
    size = model->neuronN[i] * sizeof(unsigned int);
    os << size << ", cudaMemcpyHostToDevice));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpkEvnt" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, glbSpkEvnt" << model->neuronName[i] << ", ";
    size = model->neuronN[i] * sizeof(unsigned int);
    if (model->neuronDelaySlots[i] != 1) {
      size *= model->neuronDelaySlots[i];
    }
    os << size << ", cudaMemcpyHostToDevice));" << endl;      
    for (int j= 0; j < model->inSyn[i].size(); j++) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_inSyn" << model->neuronName[i] << j << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, inSyn" << model->neuronName[i] << j << ", ";
      os << model->neuronN[i] << " * sizeof(" << model->ftype << "), cudaMemcpyHostToDevice));" << endl;
    }
    for (int k = 0, l = nModels[nt].varNames.size(); k < l; k++) {   
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_" << nModels[nt].varNames[k] << model->neuronName[i]<< ", ";
      os << nModels[nt].varNames[k] << model->neuronName[i] << ", ";
      if ((nModels[nt].varNames[k] == "V") && (model->neuronDelaySlots[i] != 1)) {
	size = model->neuronN[i] * model->neuronDelaySlots[i];
      }
      else {
	size = model->neuronN[i];
      }
      os << size << " * sizeof(" << nModels[nt].varTypes[k] << "), cudaMemcpyHostToDevice));" << endl;
    }
    if (model->neuronNeedSt[i]) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_sT" << model->neuronName[i] << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(devPtr, " << "sT" << model->neuronName[i] << ", ";
      size = model->neuronN[i] * theSize(model->ftype);
      os << size << ", cudaMemcpyHostToDevice));" << endl;
      mos << "model->receivesInputCurrent[i]: " << model->receivesInputCurrent[i] << endl;
    }
  }
  for (int i=0; i< model->postSynapseType.size(); i++){
    int pst= model->postSynapseType[i];
    for (int k= 0, l= postSynModels[pst].varNames.size(); k < l; k++) {
      
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(d_" << postSynModels[pst].varNames[k] << model->synapseName[i]<< ", ";
      os << postSynModels[pst].varNames[k] << model->synapseName[i] << ", ";
      size = model->neuronN[model->synapseTarget[i]];
      os << size << " * sizeof(" << postSynModels[pst].varTypes[k] << "), cudaMemcpyHostToDevice));" << endl;
    }
  }
  os << "}" << endl;
  os << endl;


  // ------------------------------------------------------------------------
  // copying values from device

  os << "void copyStateFromCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  os << "  void *devPtr;" << endl;
  for (int i= 0; i < model->neuronGrpN; i++) {
    nt= model->neuronType[i];
    if (model->neuronDelaySlots[i] != 1) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_spkEvntQuePtr" << model->neuronName[i] << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(&spkEvntQuePtr" << model->neuronName[i] << ", devPtr, ";
      size = sizeof(unsigned int);
      os << size << ", cudaMemcpyDeviceToHost));" << endl;
    }

    //glbscnt
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbscnt" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(&glbscnt" << model->neuronName[i] << ", devPtr, ";
    size = sizeof(unsigned int);
    os << size << ", cudaMemcpyDeviceToHost));" << endl;

    //glbSpkEvntCnt
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpkEvntCnt" << model->neuronName[i] << "));" << endl;
    if (model->neuronDelaySlots[i] == 1) {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(&glbSpkEvntCnt" << model->neuronName[i] << ", devPtr, ";
      size = sizeof(unsigned int);
    }
    else {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(glbSpkEvntCnt" << model->neuronName[i] << ", devPtr, ";
      size = model->neuronDelaySlots[i] * sizeof(unsigned int);
    }
    os << size << ", cudaMemcpyDeviceToHost));" << endl;

    //glbSpk
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpk" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(glbSpk" << model->neuronName[i] << ", devPtr, ";
    size = model->neuronN[i] * sizeof(unsigned int);
    os << size << ", cudaMemcpyDeviceToHost));" << endl;


    //glbSpkEvnt
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpkEvnt" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(glbSpkEvnt" << model->neuronName[i] << ", devPtr, ";
    size = model->neuronN[i] * sizeof(unsigned int);
    if (model->neuronDelaySlots[i] != 1) size *= model->neuronDelaySlots[i];
    os << size << ", cudaMemcpyDeviceToHost));" << endl;
    for (int j= 0; j < model->inSyn[i].size(); j++) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_inSyn" << model->neuronName[i] << j << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(inSyn" << model->neuronName[i] << j << ", devPtr, ";
      os << model->neuronN[i] << " * sizeof(" << model->ftype << "), cudaMemcpyDeviceToHost));" << endl;
    }
    for (int k= 0, l= nModels[nt].varNames.size(); k < l; k++) {
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(" << nModels[nt].varNames[k] << model->neuronName[i] << ", ";
      os << "d_" << nModels[nt].varNames[k] << model->neuronName[i] << ", ";
      if ((nModels[nt].varNames[k] == "V") && (model->neuronDelaySlots[i] != 1)) {
	size = model->neuronN[i] * model->neuronDelaySlots[i];
      }
      else {
	size = model->neuronN[i];
      }
      os << size << " * sizeof(" << nModels[nt].varTypes[k] << "), cudaMemcpyDeviceToHost));" << endl;
    }
    
    if (model->neuronNeedSt[i]) {
      os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_sT" << model->neuronName[i] << "));" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(sT" << model->neuronName[i] << ", devPtr, ";
      os << model->neuronN[i] << " * sizeof(" << model->ftype << "), cudaMemcpyDeviceToHost));" << endl;
    }
  }
  
  for (int i=0; i< model->postSynapseType.size(); i++){
    int pst= model->postSynapseType[i];
    for (int k= 0, l= postSynModels[pst].varNames.size(); k < l; k++) {
      //os << postSynModels[pst].varTypes[k] << " *";
      //os << postSynModels[pst].varNames[k] << model->synapseName[i] << ";" << endl;
      os << "  CHECK_CUDA_ERRORS(cudaMemcpy(" << postSynModels[pst].varNames[k] << model->synapseName[i] << ", ";
      os << "d_" << postSynModels[pst].varNames[k] << model->synapseName[i] << ", ";
      size = model->neuronN[model->synapseTarget[i]];
      os << size << " * sizeof(" << postSynModels[pst].varTypes[k] << "), cudaMemcpyDeviceToHost));" << endl;
    }
  }
  os << "}" << endl;
  os << endl;

  // --------------------------
  // copying spikes from device                                            

  os << "void copySpikesFromCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  os << "  void *devPtr;" << endl;
  for (int i= 0; i < model->neuronGrpN; i++) {
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbSpk" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(glbSpk" << model->neuronName[i] << ", devPtr, ";
    os << "glbscnt" << model->neuronName[i] << " * " << sizeof(unsigned int);
    os << ", cudaMemcpyDeviceToHost));" << endl;
  }
  os << "}" << endl;
  os << endl;


  // ---------------------------------
  // copying spike numbers from device

  os << "void copySpikeNFromCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  os << "  void *devPtr;" << endl;
  for (int i= 0; i < model->neuronGrpN; i++) {
    os << "  CHECK_CUDA_ERRORS(cudaGetSymbolAddress(&devPtr, d_glbscnt" << model->neuronName[i] << "));" << endl;
    os << "  CHECK_CUDA_ERRORS(cudaMemcpy(&glbscnt" << model->neuronName[i] << ", devPtr, ";
    size = sizeof(unsigned int);
    os << size << ", cudaMemcpyDeviceToHost));" << endl;
  }
  os << "}" << endl;
  os << endl;

 
  // ----------------------------------------------------
  // clean device memory that was allocated from the host

  os << "void freeMemCuda" << deviceID << "()" << endl;
  os << "{" << endl;
  for (int i= 0; i < model->neuronGrpN; i++) {
    nt= model->neuronType[i];
    for (int k= 0, l= nModels[nt].varNames.size(); k < l; k++) {
      os << "  CHECK_CUDA_ERRORS(cudaFree(d_" << nModels[nt].varNames[k] << model->neuronName[i] << "));" << endl;
    }
  }
  for (int i= 0; i < model->synapseGrpN; i++) {
    if ((model->synapseGType[i] == (INDIVIDUALG)) || (model->synapseGType[i] == INDIVIDUALID)) {
      os << "  CHECK_CUDA_ERRORS(cudaFree(d_gp" << model->synapseName[i] << "));" <<endl;  	
    }
    if (model->synapseType[i] == LEARN1SYNAPSE) {
      os << " CHECK_CUDA_ERRORS(cudaFree(d_grawp"  << model->synapseName[i] << "));" <<endl;	
    }
  }
  for (int i=0; i< model->postSynapseType.size(); i++){
    int pst= model->postSynapseType[i];
    for (int k= 0, l= postSynModels[pst].varNames.size(); k < l; k++) {
      os << "  CHECK_CUDA_ERRORS(cudaFree(d_" << postSynModels[pst].varNames[k] << model->synapseName[i] << "));" << endl;
    }
  }
  os << "}" << endl;
  os << endl;














  // ------------------------------------------------------------------------
  // the actual time stepping procedure

  os << "void stepTimeCuda" << deviceID << "(";
  for (int i = 0; i < model->neuronGrpN; i++) {
    if (model->neuronType[i] == POISSONNEURON) {
      os << "unsigned int *rates" << model->neuronName[i];
      os << ", // pointer to the rates of the Poisson neurons in grp ";
      os << model->neuronName[i] << endl;
      os << "unsigned int offset" << model->neuronName[i];
      os << ", // offset on pointer to the rates in grp ";
      os << model->neuronName[i] << endl;
    }
    if (model->receivesInputCurrent[i]>=2) {
      os << "float *d_inputI" << model->neuronName[i];
      os << ", // Explicit input to the neurons in grp ";
      os << model->neuronName[i] << endl;
    }
  }
  os << "float t)" << endl;
  os << "{" << endl;
  int trgN;
  if (model->synapseGrpN > 0) {
    os << "  if (t > 0.0) {" << endl; 
    os << "    calcSynapsesCuda" << deviceID << " <<< sGrid, sThreads >>> (";
    for (int i= 0; i < model->synapseGrpN; i++) {
      if (model->synapseGType[i] == INDIVIDUALG) {
	os << "  d_gp" << model->synapseName[i] << ", ";
      }
      if (model->synapseConnType[i] == SPARSE) {
	os << " d_gp" << model->synapseName[i] << "_ind, ";	
	os << " d_gp" << model->synapseName[i] << "_indInG, ";	
	trgN = model->neuronN[model->synapseTarget[i]];
      }
      if (model->synapseGType[i] == INDIVIDUALID){
	os << "  d_gp" << model->synapseName[i] << ",";	
      }
      if (model->synapseType[i] == LEARN1SYNAPSE) {
	os << "d_grawp"  << model->synapseName[i] << ","; 	
      }
    }
    for (int i= 0; i < model->neuronGrpN; i++) {
      nt= model->neuronType[i];
      os << " d_" << nModels[nt].varNames[0] << model->neuronName[i];  	// this is supposed to be Vm		
      if (model->needSt||i<(model->neuronGrpN-1)) {
	os << ",";
      }    		
    }
    if (model->needSt) {
      os << "t);"<< endl;
    }
    else {
      os << ");" << endl;
    }
    if (model->lrnGroups > 0) {
      os << "    learnSynapsesPostCuda" << deviceID << " <<< lGrid, lThreads >>> (";      
      for (int i= 0; i < model->synapseGrpN; i++) {
	if ((model->synapseGType[i] == INDIVIDUALG)  || (model->synapseGType[i] == INDIVIDUALID )) {
	  os << " d_gp" << model->synapseName[i] << ",";	
	}
	if (model->synapseType[i] == LEARN1SYNAPSE) {
	  os << "d_grawp"  << model->synapseName[i] << ",";	
	}
      }
      for (int i= 0; i < model->neuronGrpN; i++) {
	nt= model->neuronType[i];
	os << " d_" << nModels[nt].varNames[0] << model->neuronName[i] << ","; // this is supposed to be Vm
      }
      os << "t);" << endl;
    }
    os << "  }" << endl;
  }
  os << "  calcNeuronsCuda" << deviceID << " <<< nGrid, nThreads >>> (";
  for (int i= 0; i < model->neuronGrpN; i++) {
    nt= model->neuronType[i];
    if (nt == POISSONNEURON) {
      os << "rates" << model->neuronName[i] << ", ";
      os << "offset" << model->neuronName[i] << ",";
    }
    if (model->receivesInputCurrent[i]>=2) {
      os << "d_inputI" << model->neuronName[i] << ", ";
    }
    for (int k= 0, l= nModels[nt].varNames.size(); k < l; k++) {
      os << " d_" << nModels[nt].varNames[k] << model->neuronName[i]<< ", ";
    }
  }
  for (int i=0; i< model->postSynapseType.size(); i++){
    int pst= model->postSynapseType[i];
    for (int k= 0, l= postSynModels[pst].varNames.size(); k < l; k++) {
      os << " d_" << postSynModels[pst].varNames[k];
      os << model->synapseName[i]<< ", ";
    }
  }
  os << "t);" << endl;
  os << "}" << endl;
  osh.close();
  os.close();


  mos << "Global memory required for core model: " << mem/1e6 << " MB for all-to-all connectivity" << endl;
  mos << deviceProp[deviceID].totalGlobalMem << " this device: " << deviceID << endl;  
  if (0.5 * deviceProp[deviceID].totalGlobalMem < mem) {
    mos << "memory required for core model (" << mem/1e6;
    mos << "MB) is more than 50% of global memory on the chosen device";
    mos << "(" << deviceProp[deviceID].totalGlobalMem/1e6 << "MB)." << endl;
    mos << "Experience shows that this is UNLIKELY TO WORK ... " << endl;
  }
}
